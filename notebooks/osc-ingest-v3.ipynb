{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e0ebfc5-d93a-4731-a1a5-319b73accd2a",
   "metadata": {},
   "source": [
    "## Load 2020 WIDE-formatted ESG data (Generic)\n",
    "\n",
    "Copyright (C) 2021 OS-Climate\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "\n",
    "### Initially developed using the Royal Dutch Shell plc Sustainability Report 2020 report (Many Sheets)\n",
    "\n",
    "Contributed by Michael Tiemann (Github: MichaelTiemannOSC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4691f183-a756-45e1-9348-1156044dee52",
   "metadata": {},
   "source": [
    "Load Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74dff22-a393-47e4-bcaa-3d6c992ab083",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# From the AWS Account page, copy the export scripts from the appropriate role using the \"Command Line or Programmatic Access\" link\n",
    "# Paste the copied text into ~/credentials.env\n",
    "\n",
    "from dotenv import dotenv_values, load_dotenv\n",
    "import os\n",
    "import pathlib\n",
    "import sys\n",
    "\n",
    "dotenv_dir = os.environ.get('CREDENTIAL_DOTENV_DIR', os.environ.get('PWD', '/opt/app-root/src'))\n",
    "dotenv_path = pathlib.Path(dotenv_dir) / 'credentials.env'\n",
    "if os.path.exists(dotenv_path):\n",
    "    load_dotenv(dotenv_path=dotenv_path,override=True)\n",
    "\n",
    "import sys\n",
    "sys.path.append('../src/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee67533-80c2-4594-8e78-c864adddf1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import openpyxl\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.worksheet.dimensions import ColumnDimension, DimensionHolder\n",
    "from openpyxl.utils import get_column_letter\n",
    "from openpyxl.styles import Alignment, Font\n",
    "from itertools import islice\n",
    "\n",
    "import pint\n",
    "import pint_pandas\n",
    "import iam_units\n",
    "from openscm_units import unit_registry\n",
    "pint_pandas.PintType.ureg = unit_registry\n",
    "ureg = unit_registry\n",
    "ureg.define('fraction = [] = frac')\n",
    "ureg.define('percent = 1e-2 frac = pct = percentage')\n",
    "ureg.define('ppm = 1e-6 fraction')\n",
    "\n",
    "ureg.define(\"USD = [currency]\")\n",
    "ureg.define(\"EUR = nan USD\")\n",
    "ureg.define(\"JPY = nan USD\")\n",
    "ureg.define(\"MM_USD = 1000000 USD\")\n",
    "ureg.define(\"revenue = USD\")\n",
    "\n",
    "ureg.define(\"btu = Btu\")\n",
    "ureg.define(\"tBtu = T Btu\")\n",
    "ureg.define(\"boe = 5.712 GJ\")\n",
    "ureg.define(\"UEDCTM = [shell_index]\")\n",
    "\n",
    "ureg.define(\"CO2e = CO2 = CO2eq = CO2_eq\")\n",
    "ureg.define(\"HFC = [ HFC_emissions ]\")\n",
    "ureg.define(\"PFC = [ PFC_emissions ]\")\n",
    "ureg.define(\"mercury = Hg = Mercury\")\n",
    "ureg.define(\"PM10 = [ PM10_emissions ]\")\n",
    "\n",
    "ureg.define(\"production = [ output ]\")\n",
    "ureg.define(\"Index = pct = Share\")\n",
    "\n",
    "ureg.define(\"Number = dimensionless\")\n",
    "\n",
    "one_co2 = ureg(\"CO2e\")\n",
    "print(one_co2)\n",
    "\n",
    "from osc_ingest_trino import *\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import json\n",
    "import io\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893d0110-c719-4ae8-9fe0-248a3a4256e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ureg(\"tonnes CO2e/revenue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59df89e-d31d-4f2e-a5f2-e0d696f9e7c3",
   "metadata": {},
   "source": [
    "For spreadsheets in WIDE format, pre-process the spreadsheet as a workbook, cascading label data into 3rd-normal form row and column metadata\n",
    "\n",
    "* var_col is the label of the variable being measured (whose specificity (like CO2, CH4, NOx, etc) often affects units)\n",
    "* units_col is the column where units are stated\n",
    "* val_col:last_val_col are the column where the values are quantitatively reported\n",
    "* last_val_col+1:last_col are additional columns that are presumed to be metadata labels (such as GRI or SASB labels)\n",
    "\n",
    "We add:\n",
    "* notes_col (source worksheet-specific; could act as a kind of source table metadata)\n",
    "* topic_col (sheet-level category; if we wanted large tables, they could be named by topic)\n",
    "* category_col (to which row-level data rolls up; if we wanted small tables, they could be named by topic:category)\n",
    "* segment_col (the dimension by which row-level data is segmented)\n",
    "* units_col (if not already existing in input)\n",
    "\n",
    "Some spreadsheets use color to express a multi-level category hierarchy (such as Energy Consumption>>Business Use>>Fuel Type).  We concatenate the categories from left to right as the category for our purposes, except we split off the rightmost subcategory as the segmentation.\n",
    "\n",
    "Based on all of the above, we don't really have table-level metadata other than notes attached to sheets and generic column information.  An argument could be made that we need to allocate specifier columns for additional data we want to split out from our variables.  That could look like:\n",
    "\n",
    "* spec1_col\n",
    "* spec2_col\n",
    "\n",
    "etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a09246-078b-4ede-b891-e79fe89c8036",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# var_col = 1\n",
    "\n",
    "# Magic knowledge\n",
    "# last_col = 4\n",
    "# max_hidden_col = 5\n",
    "# year_regex = r'^(20\\d\\d) Data$'\n",
    "\n",
    "ingest_columns = [ 'Variable', 'Notes', 'Topic', 'Category', 'Segmentation', 'Unit' ]\n",
    "ingest_col_offsets = dict((j,i) for i,j in enumerate(ingest_columns[1:], start=1))\n",
    "\n",
    "# In this case, Value columns are named like 2020, 2019, 2018, ... .  It is the pd.melt function that gives us an actual Value column.\n",
    "# the val_col index merely refers to the first such value row (which hopefully has tasty data)\n",
    "\n",
    "# Magic knowledge\n",
    "# val_col = units_col+1     # units_col starts as var_col+1, val_col starts as var_col+2 which is also units_col+1\n",
    "\n",
    "# If topic_row is None, set topic based on name of sheet\n",
    "# topic = topic_row = None\n",
    "# header_row = None\n",
    "\n",
    "# If init_header_row is None, find header row based on color scheme\n",
    "# init_header_row = 1\n",
    "\n",
    "class corp_report_magic:\n",
    "    def __init__(self, shortname, input_filename, ws_start, ws_end, var_col=None, units_col=None, \n",
    "                 notes_col=None, topic_row=None, topic_col=None, category_col=None, init_header_row=None, header_row_list=None,\n",
    "                 header_color=None, cat_color_dict={ None:0 }, year_regex=None, max_hidden_col=None,\n",
    "                 val_col=None, last_val_col=None):\n",
    "        self.shortname = shortname\n",
    "        self.input_filename = '/'.join([os.environ.get('CREDENTIAL_DOTENV_DIR', os.environ.get('PWD', '/opt/app-root/src')),\n",
    "                                        'osc-ingest-shell/data/external', input_filename])\n",
    "        self.ws_start = ws_start,\n",
    "        self.ws_end = ws_end,\n",
    "        self.init_topic_row = topic_row    # If topic_row is None, use the worksheet name as the topic\n",
    "        self.var_col = var_col or 1\n",
    "        self.init_units_col = units_col            # If units_col is none, we have to allocate it\n",
    "        self.init_topic_col = topic_col    # If topic_col is non-null, we get topics from this row\n",
    "        self.init_category_col = category_col\n",
    "        self.init_notes_col = notes_col\n",
    "        self.init_val_col = val_col\n",
    "        self.init_last_val_col = last_val_col\n",
    "        # val_col, last_val_col, and last_col can be derived from the spreadsheet\n",
    "        self.units_row = -1 if units_col==None else 0   # -1: Carry across only; 0: no units seen yet; > 0 row of prevailing unit\n",
    "        self.init_header_row = init_header_row\n",
    "        self.header_row_list = header_row_list if header_row_list else ([-1] * ws_start) + ([init_header_row] * (ws_end-ws_start+1))\n",
    "        self.header_row = None\n",
    "        self.header_color = header_color\n",
    "        self.cat_color_dict = cat_color_dict\n",
    "        self.year_regex = year_regex\n",
    "        # For AEP, there are several hidden columns on the first sheet we must delete\n",
    "        # to make that sheet line up with other sheets\n",
    "        self.max_hidden_col = max_hidden_col\n",
    "        \n",
    "        self.units_col = units_col\n",
    "        self.topic_col = topic_col\n",
    "        self.category_col = category_col\n",
    "        self.notes_col = notes_col\n",
    "        self.segmentation_col = None\n",
    "        self.val_col = val_col or units_col+1 if units_col else var_col+1 if var_col else 2\n",
    "        self.last_val_col = last_val_col\n",
    "        self.last_val_row = None              # Set by preprocess (after we've identified our value columns)\n",
    "        self.last_col = None                  # Set by crop_sheet\n",
    "    \n",
    "    def preprocess(self):\n",
    "        self.wb_superscripts = None\n",
    "        self.topic_row = self.init_topic_row\n",
    "        self.units_col = self.init_units_col\n",
    "        self.topic_col = self.init_topic_col\n",
    "        self.category_col = self.init_category_col\n",
    "        self.notes_col = self.init_notes_col\n",
    "        self.segmentation_col = None\n",
    "        self.val_col = self.init_val_col or self.init_units_col+1 if self.init_units_col else self.var_col+1\n",
    "        self.last_val_col = self.init_last_val_col\n",
    "\n",
    "Shell_magic = corp_report_magic(\"Shell\", r\"greenhouse-gas-and-energy-data-shell-sr20.xlsx\", 1, 10,\n",
    "                                init_header_row=5, units_col=2)\n",
    "DPDHL_magic = corp_report_magic(\"DPDHL\", r\"DPDHL-ESG-Statbook-2020-en.xlsx\", 2, 4,\n",
    "                                topic_row=1, header_row_list=[ -1, -1, 8, 5, 4], header_color='FFBF00',\n",
    "                                cat_color_dict={ 'FF00B050':0, 'E2F0D9':1, 'D0CECE':0, 'E7E6E6':0 },\n",
    "                                units_col=2)\n",
    "Unilever_magic = corp_report_magic(\"Unilever\", r\"Unilever sustainability performance data_Climate FINAL.xlsx\", 0, 0,\n",
    "                                   topic_row=9, init_header_row=10,\n",
    "                                   cat_color_dict={'FFEBF1DE':0, 'E2F0D9':0, 'FFFFFF':2})\n",
    "AEP_magic = corp_report_magic(\"AEP\", r\"2021-Data-Centerv1.xlsx\", 0, 3,\n",
    "                              init_header_row=1,\n",
    "                              cat_color_dict={'FF237F2E':0, 'FF40B14B':1, 'FFC6E7C8':2,\n",
    "                                              'FF757575':0, 'FFBDBDBD':1, 'FFE5E5E5':2,\n",
    "                                              'FF5FB3F9':0, 'FFB9DDFC':1, 'FFE2F2FE':2,\n",
    "                                              'FFD0AF8F':0, 'FFEEDCCA':1, 'FFF2E8DE':2 },\n",
    "                              year_regex=r'^(20\\d\\d) Data$', max_hidden_col=5)\n",
    "Altria_magic = corp_report_magic(\"Altra\", r\"esg-tables.xlsx\", 1, 1,\n",
    "                                 init_header_row=2,\n",
    "                                 cat_color_dict={'FF9BDA44':0, 'FF92D050':1, 'FFE2F0D9':2, 'FFFFFFFF':2},\n",
    "                                 units_col=2)\n",
    "\n",
    "SUEZ_magic = corp_report_magic(\"SUEZ\", r\"SUEZ-FY-2020-ESG-dataset-xls-may2020.xlsx\", 1, 1,\n",
    "                               init_header_row=3,\n",
    "                               topic_col=1, category_col=2, var_col=3, units_col=4,\n",
    "                               val_col=9, last_val_col=10)\n",
    "\n",
    "filename_magic = {\n",
    "    r\"greenhouse-gas-and-energy-data-shell-sr20.xlsx\": Shell_magic,\n",
    "    r\"2021-Data-Centerv1.xlsx\": AEP_magic,\n",
    "    r\"DPDHL-ESG-Statbook-2020-en.xlsx\": DPDHL_magic,\n",
    "    r\"Unilever sustainability performance data_Climate FINAL.xlsx\": Unilever_magic,\n",
    "    r\"esg-tables.xlsx\": Altria_magic,\n",
    "}\n",
    "# A storage area in case we delete items from the above.\n",
    "foo = {\n",
    "    r\"esg-tables.xlsx\": Altria_magic,\n",
    "    r\"greenhouse-gas-and-energy-data-shell-sr20.xlsx\": Shell_magic,\n",
    "    r\"DPDHL-ESG-Statbook-2020-en.xlsx\": DPDHL_magic,\n",
    "    r\"Unilever sustainability performance data_Climate FINAL.xlsx\": Unilever_magic,\n",
    "    r\"2021-Data-Centerv1.xlsx\": AEP_magic,\n",
    "}\n",
    "\n",
    "crm = None\n",
    "value_vars = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da79214-e1e1-4fef-9222-fde04ec5e976",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_regex = re.compile(r'^((mi|bi|tri|quadri)llion|thousand|hundred)(s of)? ', flags=re.I)\n",
    "sc_xlate = {'hun':1e2, 'tho':1e3, 'mil':1e6, 'bil':1e9, 'tri':1e12, 'qua':1e15}\n",
    "\n",
    "def find_units(var):\n",
    "    scale = 1.0\n",
    "    if var in ['%', 'pct', 'percent']:\n",
    "        return 'percent'\n",
    "    if '-based' in var or 'KPI' in var:\n",
    "        return None\n",
    "    if 'Total no.' in var:\n",
    "        var = var.replace('Total no.', 'Number')\n",
    "    elif 'No.' in var:\n",
    "        var = var.replace('No.', 'Number')\n",
    "    if ' of production' in var:\n",
    "        var = var.replace(' of production', '')\n",
    "    var = var.replace ('Net MWh', 'MWh')\n",
    "    var = var.replace ('trillion (10^12)', 'trillion')\n",
    "    var = var.replace ('m3', 'kl')\n",
    "    var = var.replace ('KWh', 'kWh')\n",
    "    var = var.replace ('Index points', 'Number')\n",
    "    if var in ureg:\n",
    "        return f'{ureg(var).u:~}'\n",
    "    if var.lower() in ureg:\n",
    "        return f'{ureg(var.lower()).u:~}'\n",
    "    m = re.search(r'(((metric)|(short)) t)on', var, flags=re.I)\n",
    "    if m:\n",
    "        var = '_'.join(var.lower().split(' '))\n",
    "        if var in ureg:\n",
    "            return f'{ureg(var).u:~}'\n",
    "    m = re.search(scale_regex, var)\n",
    "    if m:\n",
    "        var = ' '.join([var[0:m.start(0)],var[m.end(0):]]).strip()\n",
    "        if var in ureg:\n",
    "            units = sc_xlate[m.group(1)[0:3].lower()] * ureg(var)\n",
    "            units = units.to_compact()\n",
    "            if units.m - 1.0 < 0.00001:\n",
    "                # Address roundoff problems such as giga = 1.00000000000002 x 10^9\n",
    "                return f'{units.u:~}'\n",
    "            print(f'units do not reduce: {units}')\n",
    "    print(f'find units: nothing found for {var}')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007e3a1b-a0a6-410a-8b6b-f921c5be2261",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "topic_keywords = { 'footprint':['intensity'],\n",
    "                   'emissions':['scope 1', 'scope 2', 'scope 3', 'ghg', 'intensity'],\n",
    "                   'energy':['consum', 'generat', 'renewable', 'intensity'],\n",
    "                   'water':['consum', 'discharge', 'withdraw', 'intensity'],\n",
    "                   'waste':['landfill', 'incinerate', 'compost', 'recycle', 'reuse', 'intensity'],\n",
    "                   'other':[]}\n",
    "\n",
    "topic_cell = None\n",
    "category_cell = None\n",
    "segmentation_stack = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed387f8-184c-469e-8e2a-21c071b08202",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_topic(ws, row):\n",
    "    \"\"\"\n",
    "    Topics are major headers.  If such headers also have units defined, they are also categories.\n",
    "    If such headers also have values defined, they are also processed as a variable.\n",
    "    \"\"\"\n",
    "    global topic_cell, category_cell\n",
    "    \n",
    "    if row==None:\n",
    "        # We have to put the topic in header_row+1 because header_row is column info, not data, for the dataframe\n",
    "        row = crm.header_row+1\n",
    "        topic_cell = ws.cell(row, crm.topic_col)\n",
    "        topic_cell.value = ws.title.lower()\n",
    "        print(f'process_topic {row}: setting topic from title {topic_cell.value}')\n",
    "\n",
    "    cell = ws.cell(row, crm.var_col)\n",
    "    if cell.value:\n",
    "        var_text = cell.value.split('\\n')[0]    # notes have been stripped out\n",
    "    else:\n",
    "        var_text = None\n",
    "    \n",
    "    if var_text:\n",
    "        topic_cell = ws.cell(row, crm.topic_col)\n",
    "        if topic_cell.value!=ws.title.lower():\n",
    "            # Let's assume topic text is not parenthetical, but titular\n",
    "            var_text = re.sub(r'\\(.+\\)', '', var_text)\n",
    "            var_words = var_text.split(' ')\n",
    "            for word in var_words:\n",
    "                if topic_cell.value and word.lower() == topic_cell.value:\n",
    "                    # Not a new topic\n",
    "                    break\n",
    "                if word.lower() in topic_keywords.keys():\n",
    "                    print(f'process_topic {row}: setting topic {word}')\n",
    "                    topic_cell.value = word.lower()\n",
    "\n",
    "            if topic_cell.value==None:\n",
    "                print(f'worksheet {ws.title}: unknown topic {var_text}')\n",
    "                topic_cell.value = ws.title.lower()\n",
    "                # topic_keywords[ws.title.lower()] = []\n",
    "\n",
    "        # Try to extract units from Variable description\n",
    "        if ws.cell(row, crm.units_col).value==None:\n",
    "            p_exprs = re.findall(r'\\((.+)\\)', ws.cell(row, crm.var_col).value)\n",
    "            for p in p_exprs:\n",
    "                if find_units(p):\n",
    "                    print(f'process_topic {row}: setting units from var: {p}')\n",
    "                    ws.cell(row, crm.units_col).value = p\n",
    "                    break\n",
    "        # Here we don't look for species_unit; mistake???\n",
    "        \n",
    "        # If we definitely have units, set the category, which will also process the variable (if needed)\n",
    "        if ws.cell(row, crm.units_col).value:\n",
    "            print(f'process_topic {row}: setting category {var_text}')\n",
    "            category_cell = ws.cell(row, crm.category_col)\n",
    "            ws.cell(row, crm.category_col).value = var_text\n",
    "\n",
    "            print(f'process_topic {row}: setting units {ws.cell(row, crm.units_col).value}')\n",
    "            units = find_units (ws.cell(row, crm.units_col).value)\n",
    "            if units == None:\n",
    "                error(f'unknown units {ws.cell(row, crm.units_col).value}')\n",
    "            ws.cell(row, crm.units_col).value = units\n",
    "            \n",
    "            row = process_categories (ws, row)\n",
    "    else:\n",
    "        print(f'process_topic {row}: no var text')\n",
    "    if row < 0 or row >= crm.last_val_row:\n",
    "        return crm.last_val_row\n",
    "    return row+1"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0e918cc0-f5ee-47da-af22-9f17a403c266",
   "metadata": {},
   "source": [
    "BORDER_NONE = None\n",
    "BORDER_DASHDOT = 'dashDot'\n",
    "BORDER_DASHDOTDOT = 'dashDotDot'\n",
    "BORDER_DASHED = 'dashed'\n",
    "BORDER_DOTTED = 'dotted'\n",
    "BORDER_DOUBLE = 'double'\n",
    "BORDER_HAIR = 'hair'\n",
    "BORDER_MEDIUM = 'medium'\n",
    "BORDER_MEDIUMDASHDOT = 'mediumDashDot'\n",
    "BORDER_MEDIUMDASHDOTDOT = 'mediumDashDotDot'\n",
    "BORDER_MEDIUMDASHED = 'mediumDashed'\n",
    "BORDER_SLANTDASHDOT = 'slantDashDot'\n",
    "BORDER_THICK = 'thick'\n",
    "BORDER_THIN = 'thin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659ec878-2e22-4330-b1da-e6c0d8fee782",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatted_as_sub(cell1, cell2):\n",
    "    color1 = cell2rgb(cell1)\n",
    "    color2 = cell2rgb(cell2)\n",
    "    if color1 in crm.cat_color_dict:\n",
    "        if color2 in crm.cat_color_dict:\n",
    "            if crm.cat_color_dict[color1] < crm.cat_color_dict[color2]:\n",
    "                return True\n",
    "            if crm.cat_color_dict[color1] > crm.cat_color_dict[color2]:\n",
    "                return False\n",
    "        else:\n",
    "            return True\n",
    "    elif color2 in crm.cat_color_dict:\n",
    "        return False\n",
    "    \n",
    "    sub_score = 0\n",
    "    if cell1.font.b and cell2.font.b==False:\n",
    "        # print('+bold')\n",
    "        sub_score += 1\n",
    "    if cell1.font.b==False and cell2.font.b:\n",
    "        # print('-bold')\n",
    "        sub_score -= 1\n",
    "    if cell1.font.u and cell2.font.u==False:\n",
    "        # print('+underline')\n",
    "        sub_score += 1\n",
    "    if cell1.font.u==False and cell2.font.u:\n",
    "        # print('-underline')\n",
    "        sub_score -= 1\n",
    "    if cell1.alignment.indent < cell2.alignment.indent:\n",
    "        # print('+indent')\n",
    "        sub_score += 1\n",
    "    elif cell1.alignment.indent > cell2.alignment.indent:\n",
    "        # print('-indent')\n",
    "        sub_score -= 1\n",
    "    if cell1.font.sz < cell2.font.sz:\n",
    "        # print('+size')\n",
    "        sub_score += 1\n",
    "    elif cell1.font.sz > cell2.font.sz:\n",
    "        # print('-size')\n",
    "        sub_score -= 1\n",
    "    if cell1.alignment.horizontal == 'left' and cell2.alignment.horizontal == 'right':\n",
    "        # print('+halign')\n",
    "        sub_score += 1\n",
    "    elif cell1.alignment.horizontal == 'right' and cell2.alignment.horizontal == 'left':\n",
    "        # print('-halign')\n",
    "        sub_score -= 1\n",
    "    # print(f'sub_score = {sub_score}')\n",
    "    if sub_score > 0:\n",
    "        return True\n",
    "    if sub_score < 0:\n",
    "        return False\n",
    "    if sub_score == 0:\n",
    "        return None\n",
    "\n",
    "def process_categories(ws, row):\n",
    "    \"\"\"\n",
    "    Categories have units\n",
    "    \"\"\"\n",
    "    global topic_cell, category_cell, segmentation_stack\n",
    "    \n",
    "    while row < crm.last_val_row:\n",
    "        cell = ws.cell(row, crm.var_col)\n",
    "        if cell.value:\n",
    "            var_text = cell.value.split('\\n')[0]    # notes have been stripped out\n",
    "        else:\n",
    "            # Some categories are declared across multiple lines, with units by themselves\n",
    "            var_text = None\n",
    "        \n",
    "        # If we're already processing this row as a topic (and being called from that context), don't recurse\n",
    "        if var_text and topic_cell.row < row:\n",
    "            color = cell2rgb(ws.cell(row,1))\n",
    "            if color and color in crm.cat_color_dict:\n",
    "                # Hack because Unilever uses colors wrongly, but punctuation saves the day\n",
    "                if crm.cat_color_dict[color]==0 and var_text[-1]!=':':\n",
    "                    # Register that we have a new topic\n",
    "                    topic_cell = ws.cell(row, crm.topic_col)\n",
    "                    topic_cell.value = var_text\n",
    "                    print(f'process_categories: new topic set at row {row}: {var_text} (color = {color})')\n",
    "                    return process_topic(ws, row)\n",
    "                if crm.cat_color_dict[color]==1:\n",
    "                    category_cell = ws.cell(row, crm.category_col)\n",
    "                    category_cell.value = var_text\n",
    "                    print(f'process_categories: new category set at row {row}: {var_text} (color = {color})')\n",
    "            elif color:\n",
    "                if color!='00000000':\n",
    "                    print(f'process_categories: unknown color {color} at row {row}: {var_text}')\n",
    "            elif None in crm.cat_color_dict:\n",
    "                category_cell = ws.cell(row, crm.category_col)\n",
    "                category_cell.value = var_text\n",
    "                print(f'process_categories: new category set at row {row}: {var_text}')\n",
    "\n",
    "        # Try to extract units from Variable description\n",
    "        if ws.cell(row, crm.units_col).value==None and var_text:\n",
    "            p_exprs = re.findall(r'\\((.+)\\)', ws.cell(row, crm.var_col).value)\n",
    "            for p in p_exprs:\n",
    "                if 'scope' in p.lower() or 'category' in p.lower():\n",
    "                    continue\n",
    "                if find_units(p):\n",
    "                    print(f'process_categories {row}: setting units from var: {p}')\n",
    "                    ws.cell(row, crm.units_col).value = p\n",
    "                    break\n",
    "        \n",
    "        # Apply our best guess for units in case we need to propagate in segmentation\n",
    "        var_units = ws.cell(row, crm.units_col).value\n",
    "        var_species = ''\n",
    "        if var_units:\n",
    "            var_units = find_units (var_units)\n",
    "            m = re.search('r\\((.+)\\)', var_text)\n",
    "            if m:\n",
    "                var_species = m.group(1)\n",
    "                species_units = find_units(' '.join([var_units, var_species]))\n",
    "                if species_units:\n",
    "                    units = species_units\n",
    "                    var_text = ' '.join([var_text[0:m.start(1)], var_text[m.end(1)+1:]]).replace('  ', ' ')\n",
    "            else:\n",
    "                units = var_units\n",
    "        elif category_cell:\n",
    "            units = ws.cell(category_cell.row, crm.units_col).value\n",
    "        else:\n",
    "            units = None\n",
    "        ws.cell(row, crm.units_col).value = units\n",
    "        \n",
    "        total_of = ''\n",
    "        if var_text and 'total' in var_text.lower():\n",
    "            c1, c2 = re.split(r'\\s*totals?\\s*', var_text, flags=re.I)\n",
    "            if c2.strip()=='':\n",
    "                total_of = c2 = c1\n",
    "                c1 = var_text\n",
    "            elif c1.strip()=='':\n",
    "                c1 = var_text\n",
    "                total_of = c2\n",
    "            else:\n",
    "                total_of = var_text\n",
    "                # we have c1:c2\n",
    "        \n",
    "        segment_by = ''\n",
    "        for x in [ ' per ', ' by ', ' of ' ]:\n",
    "            if var_text and x in var_text:\n",
    "                segment_by = x\n",
    "                break\n",
    "\n",
    "        if formatted_as_sub(ws.cell(row, crm.var_col), ws.cell(row+1, crm.var_col)):\n",
    "            if segment_by:\n",
    "                c1, c2 = var_text.split(segment_by, 1)\n",
    "            elif not total_of:\n",
    "                c1 = var_text\n",
    "                c2 = '(anon)'\n",
    "            category_cell.value = c1\n",
    "            if segment_by or total_of:\n",
    "                print(f'process_categories {row}: segmenting {c1}::{c2}')\n",
    "                segmentation_stack = [ ws.cell(row, crm.segmentation_col) ]\n",
    "                segmentation_stack[-1].value = c2\n",
    "                row = process_var(ws, row)\n",
    "                row = process_segmentation (ws, row)\n",
    "                if segmentation_stack != []:\n",
    "                    print(f'process_categories: segmentation_stack = {segmentation_stack}')\n",
    "                if row < 0:\n",
    "                    return row\n",
    "                continue\n",
    "        segmentation_stack = []\n",
    "        # print(f'process_category {row}: processing variable')\n",
    "        row = process_var(ws, row)\n",
    "        if row < 0:\n",
    "            return row\n",
    "    if row < crm.last_val_row:\n",
    "        return row+1\n",
    "    if row == crm.last_val_row:\n",
    "        process_var (ws, row)\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c35c9d9-d4ed-4b71-a5cb-95a49ce2ea20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_segmentation(ws, row):\n",
    "    \"\"\"Process rows starting at ROW as a part of a segmentation.  We push and recurse if we see\n",
    "    a new level of indentation.  We pop and return if we see an outdent.\n",
    "    \"\"\"\n",
    "    global segmentation_stack\n",
    "    \n",
    "    seg_start_cell = ws.cell(row, crm.var_col)\n",
    "    seg_start_units = ws.cell(row, crm.units_col).value\n",
    "    while row < crm.last_val_row:\n",
    "        cell = ws.cell(row, crm.var_col)\n",
    "        if cell.value==None:\n",
    "            if ws.cell(row, crm.units_col).value and ws.cell(row+1, crm.units_col).value==None:\n",
    "                ws.cell(row+1, crm.units_col).value = ws.cell(row, crm.units_col).value\n",
    "                row = row+1\n",
    "                # Don't start a segment with an unlabeled cell\n",
    "                seg_start_cell = ws.cell(row, crm.var_col)\n",
    "                continue\n",
    "            else:\n",
    "                print(f'unhandled units at row {row}: {ws.cell(row+1, crm.units_col).value}; {ws.cell(row, crm.units_col).value}')\n",
    "\n",
    "        if cell==seg_start_cell:\n",
    "            # Handle easy case\n",
    "            row = process_var (ws, row)\n",
    "        else:\n",
    "            new_seg_start = formatted_as_sub(seg_start_cell, cell)\n",
    "            if new_seg_start:\n",
    "                if ws.cell(row, crm.units_col).value==None:\n",
    "                    ws.cell(row, crm.units_col).value = seg_start_units\n",
    "                # There could be many rows at the same level as SEG_START_CELL before a new segmentation is seen\n",
    "                # The label we care about is the one immediately preceding, not the first one with that indentation\n",
    "                segmentation_stack.append(ws.cell(row-1, crm.var_col))\n",
    "                row = process_segmentation(ws, row)\n",
    "            elif new_seg_start==False:\n",
    "                segmentation_stack.pop()\n",
    "                print(f'pop at row {row}: segmentation_stack now {segmentation_stack}')\n",
    "                return row\n",
    "            else:\n",
    "                if ws.cell(row, crm.units_col).value==None and seg_start_units:\n",
    "                    ws.cell(row, crm.units_col).value = seg_start_units\n",
    "                row = process_var (ws, row)\n",
    "        if row < 0:\n",
    "            return row\n",
    "    if row < crm.last_val_row:\n",
    "        return row+1\n",
    "    if row == crm.last_val_row:\n",
    "        process_var (ws, row)\n",
    "        if segmentation_stack:\n",
    "            print(f'process_segmentation: stack at end = {segmentation_stack}')\n",
    "            segmentation_stack = []\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66de52b-8aae-443a-938a-21f1f34987ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_var(ws, row):\n",
    "    global topic_cell, category_cell, segmentation_stack\n",
    "    \n",
    "    cell = ws.cell(row, crm.var_col)\n",
    "    \n",
    "    # Treat X (Y) as 'Category X Segmentation Y'\n",
    "    if cell.value:\n",
    "        var_text = cell.value.split('\\n')[0]    # notes have been stripped out\n",
    "        var_text = var_text.replace('(%)', '(pct)')\n",
    "        m = re.search(r'^(.*) \\((.*?)\\)', var_text)\n",
    "    else:\n",
    "        var_text = None\n",
    "        m = None\n",
    "    if m and '-based' not in m.group(2) and 'scope' not in m.group(2).lower() and 'category' not in m.group(2).lower():\n",
    "        species_units = find_units (m.group(2))\n",
    "        if species_units:\n",
    "            # print(f'process_var {row}: found species or units in {var_text}')\n",
    "            units = ws.cell(row, crm.units_col).value\n",
    "            if units and units != species_units:\n",
    "                species_units = find_units(' '.join([units, m.group(2)]))\n",
    "                if species_units:\n",
    "                    var_text = m.group(1).rstrip()\n",
    "                    units = species_units\n",
    "                else:\n",
    "                    print(f'??? Not overriding {units} with {m.group(2)}')\n",
    "                    # units = ws.cell(row, crm.units_col).value\n",
    "            else:\n",
    "                units = species_units\n",
    "                if ' ' not in m.group(1) and m.group(1) in ureg:\n",
    "                    species_units = find_units(' '.join([units, m.group(1)]))\n",
    "                    if species_units:\n",
    "                        var_text = m.group(1).rstrip()\n",
    "                        units = species_units\n",
    "                # print(f'Inferring/composing units: {units}')\n",
    "            if units != ws.cell(category_cell.row, crm.units_col).value:\n",
    "                # print(f'changing units from category: {ws.cell(category_cell.row, crm.units_col).value} to {units}')\n",
    "                ws.cell(row, crm.units_col).value = units\n",
    "        elif m.group(2).lower() and topic_cell.value.lower() in topic_keywords and m.group(2).lower() in topic_keywords[topic_cell.value.lower()]:\n",
    "            # Scope 1 is actually a sneaky segmentation\n",
    "            category_cell = ws.cell(row, crm.category_col)\n",
    "            category_cell.value = m.group(2)\n",
    "        else:\n",
    "            print(f'process_var {row}: unhandled ( {m.group(2)} )')\n",
    "    else:\n",
    "        if ws.cell(row, crm.units_col).value == None:\n",
    "            # print(f'process_var {row}: propagating units {ws.cell(category_cell.row, crm.units_col).value}')\n",
    "            ws.cell(row, crm.units_col).value = ws.cell(category_cell.row, crm.units_col).value\n",
    "        else:\n",
    "            # print(f'process_var {row}: using units {ws.cell(row, crm.units_col).value}')\n",
    "            pass\n",
    "    ws.cell(row, crm.topic_col).value = topic_cell.value\n",
    "    ws.cell(row, crm.category_col).value = category_cell.value\n",
    "    if segmentation_stack != []:\n",
    "        ws.cell(row, crm.segmentation_col).value = '::'.join(s.value for s in segmentation_stack)\n",
    "    if row < crm.last_val_row:\n",
    "        return row+1\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4b839c-1347-4459-ba03-64510dc67ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ??? The header row color is going to be spreadsheet-specific.  This is what DPDHL gives us.\n",
    "\n",
    "import cell2rgb\n",
    "from cell2rgb import cell2rgb\n",
    "\n",
    "def find_header_row(wb, ws):\n",
    "    # If we haven't found the header by max_row-1, we'll never find it...\n",
    "    for row in range(1, ws.max_row):\n",
    "        color = cell2rgb(ws.cell(row,1))\n",
    "        if color == crm.header_color:\n",
    "            return row\n",
    "        print(f'find_header_row: color = {color}')\n",
    "    error('No header found')\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2927f878-eade-47b7-81b7-dea1b10de622",
   "metadata": {},
   "source": [
    "# The role of this function is to capture and distrbute data attributes that can be inferred/applied to subsequent rows\n",
    "def split_cell(c):\n",
    "    notes = ''\n",
    "\n",
    "    # Deal with None\n",
    "    if c.value:\n",
    "        h = str(c.value)\n",
    "    else:\n",
    "        h = ''\n",
    "\n",
    "    # Don't let 'Scope 1' look like a note\n",
    "    m = re.search(r'[^ ](\\d+)$', h)\n",
    "    if m:\n",
    "        notes = m.group(1)\n",
    "        h = h[0:m.start(1)]\n",
    "    else:\n",
    "        m = re.search(r'\\s*\\[[A-Z]\\](\\s?\\[[A-Z]\\])*', h)\n",
    "        if m:\n",
    "            notes = m.group(0)\n",
    "            h = h[0:m.start(0)].strip()\n",
    "\n",
    "    # If we have a ':' in the cell, treat that as a major connector that overrides splitting on per/by/of\n",
    "    if ':' not in h:\n",
    "        # If the variable expresses a segmentation, pass that back accordingly\n",
    "        for x in [ ' per ', ' by ', ' of ' ]:\n",
    "            sub_h_arr = h.split(x, 1)\n",
    "            if len(sub_h_arr)>1:\n",
    "                return notes, sub_h_arr[0], sub_h_arr[1]\n",
    "\n",
    "    # Treat X (Y) as 'Category X Segmentation Y'\n",
    "    m = re.search(r'^(.*) \\((.*?)\\)', h)\n",
    "    if m:\n",
    "        return notes, m.group(1), m.group(2)\n",
    "    return notes, h, ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43609bc3-b405-417c-95fd-c07652e290f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "from lxml import etree\n",
    "import xml.etree.ElementTree as eTree\n",
    "\n",
    "# We pre-process the structure of the worksheet so that it can be trivially loaded into a dataframe for further reshaping.\n",
    "\n",
    "# Stash notes for each worksheet here.  These are *per worksheet*\n",
    "# ??? In the case of DPDHL, there's a Comment field we don't track, which means we miss a stated target\n",
    "ws_notes = {}\n",
    "\n",
    "def preprocess(wb, ws):\n",
    "    global crm, value_vars\n",
    "    global topic_cell, category_cell, segmentation_stack\n",
    "    \n",
    "    cell_notes_text = []\n",
    "    cell_notes_cells = []\n",
    "    \n",
    "    def crop_sheet(ws):\n",
    "        global crm\n",
    "        # Frist, set max_row/max_column based on actually active cells, not cells with random spaces or empty strings\n",
    "        this_max_row = 1\n",
    "        this_max_col = 1\n",
    "        for row in range(1,ws.max_row+1):\n",
    "            row_max_col = None\n",
    "            for col in range(1,ws.max_column+1):\n",
    "                cell = ws.cell(row,col)\n",
    "                if cell.value==None:\n",
    "                    continue\n",
    "                if type(cell.value)==str and cell.value.strip()=='':\n",
    "                    cell.value = None\n",
    "                    continue\n",
    "                if col > this_max_col:\n",
    "                    this_max_col = col\n",
    "                row_max_col = col\n",
    "            if row_max_col:\n",
    "                this_max_row = row\n",
    "        print('crop_sheet')\n",
    "        print('{} x {}'.format(ws.max_row, ws.max_column))\n",
    "        ws.delete_rows(this_max_row+1,ws.max_row)\n",
    "        ws.delete_cols(this_max_col+1,ws.max_column)\n",
    "        print('{} x {}'.format(ws.max_row, ws.max_column))\n",
    "        crm.last_col = ws.max_column\n",
    "        \n",
    "    def preprocess_notes():\n",
    "        global crm\n",
    "        z = zipfile.ZipFile(crm.input_filename)\n",
    "\n",
    "        if crm.wb_superscripts==None:\n",
    "            with z.open('xl/sharedStrings.xml') as fp:\n",
    "                ss_xml = etree.fromstring(fp.read())\n",
    "            # get the namespaces                                                                                                                                                                                                                                             \n",
    "            ssns = ss_xml.nsmap\n",
    "            if None in ssns:\n",
    "                ssns['none'] = ssns.pop(None)\n",
    "            crm.text_list = ss_xml.xpath('//none:si', namespaces=ssns)\n",
    "            # All shared strings across all sheets with superscripts                                                                                                                                                                                                                           \n",
    "            crm.wb_superscripts = [s for s in range(len(crm.text_list))\n",
    "                                   if 'superscript' in eTree.tostring(crm.text_list[s], encoding='unicode')]\n",
    "        \n",
    "        with z.open(f'xl/worksheets/sheet{wb.worksheets.index(ws)+1}.xml') as fp:\n",
    "            ws_xml = etree.fromstring(fp.read())\n",
    "        z.close()\n",
    "\n",
    "        # get the namespaces                                                                                                                                                                                                                                             \n",
    "        wsns = ws_xml.nsmap\n",
    "        if None in wsns:\n",
    "            wsns['none'] = wsns.pop(None)\n",
    "        cell_list = ws_xml.xpath('//none:c', namespaces=wsns)\n",
    "\n",
    "        # Dictionary of cells:shared strings with superscripts within this sheet's cells                                                                                                                                                                                                          \n",
    "        sheet_ss_dict = {c:s for c in range(len(cell_list))\n",
    "                         for s in crm.wb_superscripts if f' t=\"s\"><ns0:v>{s}</ns0:v></ns0:c>' in eTree.tostring(cell_list[c],\n",
    "                                                                                                                 encoding='unicode')}\n",
    "\n",
    "        cell_notes_text = []\n",
    "        cell_notes_cells = []\n",
    "        for c, s in sheet_ss_dict.items():\n",
    "            cell_name = eTree.tostring(cell_list[c], encoding='unicode').split(' ')[2].split('=')[1][1:-1]\n",
    "            cell_text_xml = eTree.tostring(crm.text_list[s],encoding='unicode')\n",
    "            ss_bool = ['superscript' in x for x in cell_text_xml.split('<ns0:r>')[1:]]\n",
    "            cell_text_parts = [re.split('<ns0:t.*?>', x)[-1].split('</ns0:t>')[0]\n",
    "                  for x in cell_text_xml.split('<ns0:r>')[1:]]\n",
    "            for x in range(len(ss_bool)-1):\n",
    "                if ss_bool[x]==False and ss_bool[x+1]==True:\n",
    "                    cell_notes_text.append(cell_text_parts[x+1])\n",
    "                    cell_notes_cells.append(cell_name)\n",
    "                    ws[cell_name].value = ws[cell_name].value.replace(cell_text_parts[x]+cell_text_parts[x+1],\n",
    "                                                                      cell_text_parts[x])\n",
    "    \n",
    "    crm.preprocess()\n",
    "    preprocess_notes()\n",
    "    print(cell_notes_text)\n",
    "    print(cell_notes_cells)\n",
    "    \n",
    "    topic_cell = None\n",
    "    category_cell = None\n",
    "    segmentation_stack = []\n",
    "\n",
    "    # Remove merged cells\n",
    "    mergedRanges=ws.merged_cells.ranges\n",
    "    while mergedRanges:\n",
    "        for entry in mergedRanges:\n",
    "            ws.unmerge_cells(str(entry))\n",
    "\n",
    "    if crm.max_hidden_col and wb.worksheets[0]==ws:\n",
    "        ws.delete_cols(1,crm.max_hidden_col)\n",
    "    crop_sheet(ws)\n",
    "\n",
    "    if crm.init_header_row:\n",
    "        crm.header_row = crm.init_header_row\n",
    "    else:\n",
    "        crm.header_row = find_header_row (wb, ws)\n",
    "    \n",
    "    # Reset this for each worksheet\n",
    "    if crm.units_row >= 0:\n",
    "        crm.units_row = 0\n",
    "\n",
    "    col = crm.val_col\n",
    "    last_val_col = crm.last_val_col or col\n",
    "    while crm.last_val_col==None or col<=crm.last_val_col:\n",
    "        # ??? Deal with note in header value (such as '2019(b)' or, God forbit '20197' where the superscripted 7 just sits like it's part of the number)\n",
    "        if crm.year_regex:\n",
    "            maybe_year = re.sub(crm.year_regex, r'\\1', str(ws.cell(crm.header_row, col).value))\n",
    "        else:\n",
    "            maybe_year = str(ws.cell(crm.header_row, col).value)\n",
    "        if len(maybe_year)>=4 and maybe_year[0:2]=='20' and maybe_year[2].isdigit() and maybe_year[3].isdigit():\n",
    "            ws.cell(crm.header_row, col).value = maybe_year[0:4]\n",
    "            last_val_col = col\n",
    "        elif crm.last_val_col==None:\n",
    "            crm.last_val_col = last_val_col\n",
    "            break\n",
    "        col = col+1\n",
    "    value_vars = [ None ] * (crm.last_val_col-crm.val_col+1)\n",
    "    for col in range(crm.val_col, crm.last_val_col+1):\n",
    "        value_vars[col-crm.val_col] = ws.cell(crm.header_row, col).value\n",
    "    print(value_vars)\n",
    "    \n",
    "    # Make space for TOPIC : CATEGORY : SEGMENTATION triple.\n",
    "    # This triple could very well become an index into a data framework (such as SASB, TCFD, etc)\n",
    "    new_column_count = (len(ingest_columns)-1\n",
    "                        -int(crm.notes_col!=None)\n",
    "                        -int(crm.topic_col!=None)\n",
    "                        -int(crm.category_col!=None)\n",
    "                        -int(crm.units_col!=None))\n",
    "    ws.insert_cols(crm.last_val_col+1,amount=new_column_count)\n",
    "    if crm.notes_col==None:\n",
    "        crm.notes_col = crm.last_val_col+ingest_col_offsets['Notes']\n",
    "    ws.cell(crm.header_row,crm.notes_col).value = 'Notes'\n",
    "    if crm.topic_col==None:\n",
    "        crm.topic_col = crm.last_val_col+ingest_col_offsets['Topic']\n",
    "    ws.cell(crm.header_row,crm.topic_col).value = 'Topic'\n",
    "    if crm.category_col==None:\n",
    "        crm.category_col = crm.last_val_col+ingest_col_offsets['Category']\n",
    "    ws.cell(crm.header_row,crm.category_col).value = 'Category'\n",
    "    crm.segmentation_col = crm.last_val_col+ingest_col_offsets['Segmentation']\n",
    "    ws.cell(crm.header_row,crm.segmentation_col).value = 'Segmentation'\n",
    "    if crm.units_col==None:\n",
    "        crm.units_col = crm.last_val_col+ingest_col_offsets['Unit']\n",
    "    ws.cell(crm.header_row,crm.units_col).value = 'Unit'\n",
    "    ws.cell(crm.header_row,crm.var_col).value = 'Variable'\n",
    "        \n",
    "    crm.last_col = crm.last_col + new_column_count\n",
    "\n",
    "    # Find last row of actual values so we can process notes at the end\n",
    "    for row in range(ws.max_row, 0, -1):\n",
    "        if any([True for col in range(crm.val_col, crm.last_val_col+1) if ws.cell(row, col).value]):\n",
    "            crm.last_val_row = row\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34019c7e-8467-4b37-b0e2-96b13bacd7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(wb, ws):\n",
    "    \n",
    "    # Intended for Shell notes\n",
    "    def save_ws_notes(ws, note):\n",
    "        global ws_notes\n",
    "        \n",
    "        if ws.title not in ws_notes:\n",
    "            ws_notes[ws.title] = {}\n",
    "        note_label, note_text = note.split(' ', 1)\n",
    "        ws_notes[ws.title][note_label] = note_text.strip()\n",
    "    \n",
    "    # Intended for DPDHL notes\n",
    "    def save_ws_notes2(ws, note):\n",
    "        global ws_notes\n",
    "        \n",
    "        if ws.title not in ws_notes:\n",
    "            ws_notes[ws.title] = {}\n",
    "        notes = re.split(r' (\\d+)\\)\\s+', note)\n",
    "        print('NOTES')\n",
    "        print(notes)\n",
    "        print('END NOTES')\n",
    "        ws_notes[ws.title]['0'] = notes[0]\n",
    "        for i in range(int(len(notes)/2)):\n",
    "            ws_notes[ws.title][notes[1+2*i]] = notes[2+2*i].strip()\n",
    "    \n",
    "    # Used for Unilever\n",
    "    def finish_notes(row):\n",
    "        print('finish_notes @ {}'.format(row))\n",
    "\n",
    "    for row in range(crm.last_val_row+1, ws.max_row+1):\n",
    "        cell = ws.cell(row, crm.var_col)\n",
    "        # Find either bracketed note or note that begins with possible superscript\n",
    "        if cell.value==None:\n",
    "            continue\n",
    "        if cell.value[0]=='[':\n",
    "            save_ws_notes(ws, cell.value)\n",
    "            continue\n",
    "        if re.search(r'^[^(]*\\d[)]', str(cell.value)):\n",
    "            save_ws_notes2(ws, cell.value)\n",
    "        if re.search(r'notes', str(cell.value), flags=re.I):\n",
    "            finish_notes(row)\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7aa4190-7ea4-4f5b-afab-cc055c0d252a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With a nicely formatted workbook, do the rest of our work (including writing to Trino) using dataframes\n",
    "\n",
    "# IPIECA, SASB, and GRI columns all feed metadata\n",
    "\n",
    "def ws_to_df(wb, i):\n",
    "    data = islice(wb.worksheets[i].values, crm.header_row_list[i]-1, None)\n",
    "    cols = list(next(data))\n",
    "    data = list(data)\n",
    "    # idx = [r[0] for r in data]\n",
    "    # data = (islice(r, 0, None) for r in data)\n",
    "    cols[crm.units_col-1] = 'Unit'            # Already set by Shell; DF indexes are XLSX-1\n",
    "    df = pd.DataFrame(data, columns=cols) # we don't pass in an index here\n",
    "\n",
    "    # Remove null columns\n",
    "    df = df[[c for c in df.columns if c!= None]]\n",
    "    \n",
    "    # For now, do not remove rows lacking units.  Those are basically where Notes are stored (for better or worse).\n",
    "    # print('rows lacking proper Units')\n",
    "    # display(df[df['Unit'].isnull()])\n",
    "    df = df.loc[df.Unit.notna() | df.Category.isna()]\n",
    "\n",
    "    # Clear out data that is n/a, n/c (not collected), n/d (not disclosed)\n",
    "    df[df['Unit'].notna()].replace(to_replace='^n/[acd]$', value='', regex=True, inplace=True)\n",
    "    \n",
    "    # Change numerical years to strings to make pandas indexing behave\n",
    "    df.columns = [str(c) for c in df.columns]\n",
    "    # Drop completely empty rows\n",
    "    # df.dropna(how='all', axis=0, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63095f7-55e2-4eef-b1d4-379fcf9ebaba",
   "metadata": {},
   "source": [
    "Write out polymorphic dataframe in LONG format.  This follows tidy data model, with one variable observation per row.  \n",
    "Polymorphic means that Units/dimensions of each row are specified, but not necessarily the same row to row.  \n",
    "Aggregation functions must be careful that selection criteria does not mix up incompatible unit types and/or observation variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ffed37-1d28-4038-8390-d2f5ec91f1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "wb = None\n",
    "ws = None\n",
    "melted_df = None\n",
    "\n",
    "def ingest_filename(filename):\n",
    "    global melted_df\n",
    "    global topic_cell\n",
    "    global crm, wb, ws\n",
    "    \n",
    "    crm = filename_magic[filename]\n",
    "\n",
    "    wb = load_workbook(crm.input_filename, data_only=True)\n",
    "\n",
    "    # For a label like \"Scope 1 emissions by country\" return ['', 'Scope 1 emissions', 'country']\n",
    "    # For a label like \"Direct GHG emissions (Scope 1)[A][B] [C] [D]\" return ['[A][B] [C] [D]', 'Direct GHG emissions (Scope 1)', '']\n",
    "\n",
    "    long_fmt_filename = ''\n",
    "    wide_fmt_filename = ''\n",
    "\n",
    "    for i in range(crm.ws_start[0], crm.ws_end[0]+1):\n",
    "        ws = wb.worksheets[i]\n",
    "        ws_notes = {}\n",
    "        preprocess(wb, ws)\n",
    "        \n",
    "        row = process_topic(ws, crm.topic_row)\n",
    "\n",
    "        while row < crm.last_val_row:\n",
    "            if row == crm.header_row_list[i]:\n",
    "                row = row+1\n",
    "                continue\n",
    "            color = cell2rgb(ws.cell(row,1))\n",
    "            if color and color in crm.cat_color_dict:\n",
    "                if crm.cat_color_dict[color]==0:\n",
    "                    # Register that we have a new topic\n",
    "                    topic_cell = ws.cell(row, crm.topic_col)\n",
    "                    topic_cell.value = ws.cell(row, crm.var_col).value\n",
    "                    print(f'new topic set at row {row}: {topic_cell.value} (color = {color})')\n",
    "                    row = process_topic(ws, row)\n",
    "                else:\n",
    "                    row = process_categories(ws, row)\n",
    "                    if row < 0:\n",
    "                        break\n",
    "            else:\n",
    "                print(f'ingest_file: processing {row}')\n",
    "                row = process_topic(ws, row)\n",
    "    \n",
    "        # process_topic (wb, ws, header_row_list[i])\n",
    "        # preprocess2(wb, ws)\n",
    "        postprocess(wb, ws)\n",
    "        # What to do with ws_notes???\n",
    "        df = ws_to_df(wb, i)\n",
    "        df.replace('',pd.NA,inplace=True)\n",
    "        print(f'wb({i}) dataframe')\n",
    "        # display(df.loc[0:min(len(df),45)])\n",
    "        melted_df = pd.melt(df, id_vars=ingest_columns, var_name='Year', value_name='Value', value_vars=value_vars)\n",
    "        melted_df.dropna(subset=['Value'],inplace=True)\n",
    "        melted_df = melted_df.astype({'Year': 'int'})\n",
    "\n",
    "        if i==crm.ws_start[0]:\n",
    "            report_year = max(df.columns[crm.val_col-1:crm.last_val_col])\n",
    "            long_fmt_filename = ''.join([os.environ.get('PWD', '/opt/app-root/src'), '/osc-ingest-shell/data/interim/',\n",
    "                                         crm.shortname, '_', report_year, '_', 'LONG.xlsx'])\n",
    "            writer_long = pd.ExcelWriter(long_fmt_filename)\n",
    "            wide_fmt_filename = ''.join([os.environ.get('PWD', '/opt/app-root/src'), '/osc-ingest-shell/data/interim/',\n",
    "                                         crm.shortname, '_', report_year, '_', 'WIDE.xlsx'])\n",
    "            writer_wide = pd.ExcelWriter(wide_fmt_filename)\n",
    "\n",
    "        # This writes out LONG data with TOPIC as SHEET_NAME.  Later we'll create a truly long table with TOPIC restored as a column\n",
    "        melted_df.loc[:, melted_df.columns != 'Topic'].to_excel(writer_long, index=False, sheet_name=df.iloc[0]['Topic'][0:30])\n",
    "\n",
    "        print(ws.title)\n",
    "        columns = ['Variable', 'Unit']\n",
    "        # We need these columns to reshape our data\n",
    "        for extra_col in ['Notes', 'Category', 'Segmentation']:\n",
    "            if df[extra_col].notna().any():\n",
    "                columns.append(extra_col)\n",
    "        # In the case of Shell, we have only one topic per sheet, so can transform melted_df directly\n",
    "        pf = melted_df.pivot(index=['Year', 'Topic'], columns=columns, values=['Value'])\n",
    "        pf = pf.droplevel('Topic')\n",
    "        # Once reshaped, the extra columns actually appear as multi-level indexes.  Drop them from also behaving like values\n",
    "        pf[[c for c in columns if c not in ['Variable', 'Unit']]] = pd.NA\n",
    "        pf.dropna(how='all', axis=1, inplace=True)\n",
    "        pf.to_excel(writer_wide, sheet_name=df.iloc[0]['Topic'][0:30])\n",
    "\n",
    "    writer_long.close()\n",
    "    writer_wide.close()\n",
    "    \n",
    "    # We are now working with our own workbook, which doesn't have a zero-index sheet to ignore\n",
    "    # Make the workbook more legible to those reading it\n",
    "    long_wb = load_workbook(long_fmt_filename, data_only=True)\n",
    "    for ws in long_wb.worksheets:\n",
    "        dim_holder = DimensionHolder(worksheet=ws)\n",
    "        for col in range(ws.min_column, ws.max_column + 1):\n",
    "            if get_column_letter(col)=='A':\n",
    "                width = 40\n",
    "            elif get_column_letter(col) in ['B', 'E']:\n",
    "                width = 15\n",
    "            elif get_column_letter(col) in ['C', 'D']:\n",
    "                width = 25\n",
    "            else:\n",
    "                width = 10\n",
    "            dim_holder[get_column_letter(col)] = ColumnDimension(ws, min=col, max=col, width=width)\n",
    "        ws.column_dimensions = dim_holder\n",
    "    \n",
    "    long_wb.save(long_fmt_filename)\n",
    "    long_wb.close()\n",
    "    \n",
    "    def as_text(value):\n",
    "        if value is None:\n",
    "            return \"\"\n",
    "        return str(value)\n",
    "    \n",
    "    # Write out dataframe in WIDE format.  This data is technically tidy, with one multi-dimensional observation per row.\n",
    "    # Units/dimensions are consistent on a per-column basis, making it easy to aggregate column-based data.\n",
    "    wide_wb = load_workbook(wide_fmt_filename, data_only=True)\n",
    "    # Make the workbook more legible to those reading it\n",
    "    for ws in wide_wb.worksheets:\n",
    "        dim_holder = DimensionHolder(worksheet=ws)\n",
    "        for col in range(ws.min_column, ws.max_column + 1):\n",
    "            cell = ws.cell(2, col)\n",
    "            cell.alignment = Alignment(wrap_text=True,vertical='top') \n",
    "            dim_holder[get_column_letter(col)] = ColumnDimension(ws, min=col, max=col, width=max(10,1+len(as_text(cell.value))/3))\n",
    "        ws.column_dimensions = dim_holder\n",
    "\n",
    "    wide_wb.save(wide_fmt_filename)\n",
    "    wide_wb.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09fedea-25f5-4559-b8e6-b7b12ec6a0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in filename_magic:\n",
    "    print(filename)\n",
    "    crm = filename_magic[filename]\n",
    "    ingest_filename(filename)\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b859ab-7d24-428b-8cc9-96e1ab09f2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dff508-d294-4b92-94b6-8ea2c546df92",
   "metadata": {},
   "outputs": [],
   "source": [
    "ws.cell(11,1).border.top.style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b389459-bd1c-4bf0-b584-183e23c409f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Variable', 'Unit']\n",
    "# We need these columns to reshape our data\n",
    "for extra_col in ['Notes', 'Category', 'Segmentation']:\n",
    "    if melted_df[extra_col].notna().any():\n",
    "        columns.append(extra_col)\n",
    "melted_df[melted_df['Segmentation']=='(anon)::Road transport']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460ecd19-48a4-4781-b96e-6dac13a6b6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "units = 1e6 * ureg('kl/year')\n",
    "units.to_compact()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72a405a-8d21-42f7-ac61-9b6c5f9e2011",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bletch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da11503d-24dd-4d87-a57c-ca0355715485",
   "metadata": {},
   "outputs": [],
   "source": [
    "crm = filename_magic['DPDHL-ESG-Statbook-2020-en.xlsx']\n",
    "wb = load_workbook(crm.input_filename, data_only=True)\n",
    "ws = wb.worksheets[2]\n",
    "preprocess(wb, ws)\n",
    "\n",
    "row = process_topic(pc, crm.topic_row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381d4733-e25f-4746-9fcf-270483363f62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess2(wb, ws):\n",
    "    global crm\n",
    "    \n",
    "    scope1_gases = ['CO2', 'CH4', 'N2O', 'HFC', 'SF6', 'PFC', 'NF3', 'CO2e', 'NOx', 'SO2', 'PM10']\n",
    "    scope1_regex = re.compile('(' + ')|('.join(scope1_gases) + ')', flags=re.I)\n",
    "    \n",
    "    scope3_dict = { 'Purchased Goods and Services':1,\n",
    "                    'Capital Goods':2,\n",
    "                    'Fuel and Energy Related Activities':3,\n",
    "                    'Fuel and Energy Related Activities (Market-Based)':3,\n",
    "                    'Fuel and Energy Related Activities (Location-Based)':3,\n",
    "                    'Upstream Transportation and Distribution':4,\n",
    "                    'Transportation services':4,                # DPDHL\n",
    "                    'Fuel- and energy-related activities':4,    # DPDHL\n",
    "                    'Waste Generated in Operations (Large office campuses)':5,\n",
    "                    'Business Travel':6,\n",
    "                    'Employee Commuting':7,\n",
    "                    'Upstream Leased Assets':8,\n",
    "                    'Downstream Transportation and Distribution':9,\n",
    "                    'Processing of Sold Products':10,\n",
    "                    'Use of Sold Products':11,\n",
    "                    'End of Life Treatment of Sold Products':12,\n",
    "                    'Downstream Leads Assets':13,\n",
    "                    'Franchises':14,\n",
    "                    'Investments':15 }\n",
    "\n",
    "    # Convert reported units to things standard in `pint`\n",
    "    unit_dict = { 'trillion (10^12) MJ':'PJ', 'million MWh':'TWh', 'MW':'MW', \n",
    "                  'million tonnes CO2e':'Mt CO2e', 'tonnes CO2e':'t CO2e', 'm t CO2e':'Mt CO2e', 'MT CO2e':'Mt CO2e',\n",
    "                  'million tonnes':'Mt', 'thousand tonnes':'kilot', 'tonnes':'t', 'kg':'kg', 'MT':'Mt', 'Lbs':'lbs', 'Metric Tons':'t', \n",
    "                  'tBtu':'TBtu',\n",
    "                  'm liter':'M liter', 'Grams per  revenue':'Grams / EUR',\n",
    "                  'Millions of m3':'1000 dam', 'm3':'m3', 'Gallons':'gal',\n",
    "                  'Million Gallons':'M gallons', 'Billions of Liters':'10^9 l', 'billions of Liters':'10^9 l',}\n",
    "    u2u_dict = { '%':'pct', 'Grams per  revenue':'Grams / EUR', 'revenue':'EUR', 'MM$ revenue':'1000000 revenue',\n",
    "                 'short ton':'short_ton', 'No.':'[]', 'Nb':'[]', }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d95790-9edb-48ff-b5c3-fe4b2e37ca03",
   "metadata": {},
   "source": [
    "### Time for a Pint!\n",
    "\n",
    "See https://github.com/IAMconsortium/units/issues/9\n",
    "and https://github.com/openscm/openscm-units/issues/31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9369e4-b14f-42cd-9b28-2f906092b2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pint_pandas\n",
    "from openscm_units import unit_registry\n",
    "\n",
    "pint_pandas.PintType.ureg = u = unit_registry\n",
    "\n",
    "one_co2 = unit_registry(\"CO2\")\n",
    "print(one_co2)\n",
    "\n",
    "x = pd.DataFrame([[2.0,'Mt CO2']], columns=['Value', 'Unit'])\n",
    "print(x)\n",
    "x = x.astype({'Value': 'pint[Mt CO2]'})\n",
    "print(x.Value.pint.to('t CO2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39945448-35db-4179-ba8c-0d2cddb0e54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "u('Mt/1000000').to_compact()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0524e262-0708-41c9-9348-8815dbe0a5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "PA_ = pint_pandas.PintArray\n",
    "\n",
    "ureg = unit_registry\n",
    "Q_ = ureg.Quantity"
   ]
  },
  {
   "cell_type": "raw",
   "id": "31d04f51-689a-4022-aaa7-77206d273458",
   "metadata": {},
   "source": [
    "PA_ = pint_pandas.PintArray\n",
    "\n",
    "ureg = pint.UnitRegistry()\n",
    "Q_ = ureg.Quantity\n",
    "\n",
    "pint_pandas.PintType.ureg = ureg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626bf3a7-bfca-4329-b503-0961841a52a8",
   "metadata": {},
   "source": [
    "Note that pint[unit] must be used for the Series constuctor, whereas the PintArray constructor allows the unit string or object.\n",
    "\n",
    "```\n",
    "    df = pd.DataFrame({\n",
    "        \"length\" : pd.Series([1.,2.], dtype=\"pint[m]\"),\n",
    "        \"width\" : PA_([2.,3.], dtype=\"pint[m]\"),\n",
    "        \"distance\" : PA_([2.,3.], dtype=\"m\"),\n",
    "        \"height\" : PA_([2.,3.], dtype=ureg.m),\n",
    "        \"depth\" : PA_.from_1darray_quantity(Q_([2,3],ureg.m)),\n",
    "    })\n",
    "```\n",
    "\n",
    "See https://pint.readthedocs.io/en/0.18/pint-pandas.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac07843c-4fb7-4fd1-b124-43acffea39b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "wb = load_workbook(long_fmt_filename, data_only=True)\n",
    "\n",
    "from itertools import islice\n",
    "\n",
    "def long_ws_to_df(ws):\n",
    "    data = ws.values\n",
    "    cols = next(data)\n",
    "    data = list(data)\n",
    "    # idx = [r[0] for r in data]\n",
    "    # data = (islice(r, 1, None) for r in data)\n",
    "    \n",
    "    df = pd.DataFrame(data, columns=cols)\n",
    "\n",
    "    # The original data has topic we construct.  It is removed when writing LONG data but can be restored from SHEET_NAME\n",
    "    if 'Topic' not in df.columns:\n",
    "        print('Restoring Topic ' + ws.title)\n",
    "        df.insert(crm.topic_col-1, 'Topic', ws.title)\n",
    "    \n",
    "    return df\n",
    "\n",
    "trino_df = pd.concat([long_ws_to_df(ws) for ws in wb.worksheets])\n",
    "    \n",
    "len(trino_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdd2879-1246-4faf-b229-b102dd971acc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(trino_df['Unit'].value_counts())\n",
    "trino_df.Unit.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68075088-fa8f-446c-a377-1dc65cf34290",
   "metadata": {},
   "source": [
    "Now create data in Trino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f8bb0c-a4b4-4028-9d27-8008582bbe1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Create an S3 client.  We will user later when we write out data and metadata\n",
    "s3 = boto3.client(\n",
    "    service_name=\"s3\",\n",
    "    endpoint_url=os.environ['S3_DEV_ENDPOINT'],\n",
    "    aws_access_key_id=os.environ['S3_DEV_ACCESS_KEY'],\n",
    "    aws_secret_access_key=os.environ['S3_DEV_SECRET_KEY'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937708e8-adcf-459e-8aec-d855c53a07ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import trino\n",
    "\n",
    "conn = trino.dbapi.connect(\n",
    "    host=os.environ['TRINO_HOST'],\n",
    "    port=int(os.environ['TRINO_PORT']),\n",
    "    user=os.environ['TRINO_USER'],\n",
    "    http_scheme='https',\n",
    "    auth=trino.auth.JWTAuthentication(os.environ['TRINO_PASSWD']),\n",
    "    verify=True,\n",
    ")\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Show available schemas to ensure trino connection is set correctly\n",
    "cur.execute('show schemas in osc_datacommons_dev')\n",
    "cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9956592b-0ce9-4716-963e-4e3823b17fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "# datetime.datetime.now()\n",
    "# For now we used a fixed date so we don't fill things up needlessly\n",
    "timestamp = \"2008-09-03T20:56:35.450686Z\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef1c7de-7caf-4772-a497-610394df13cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ingest_uuid = str(uuid.uuid4())\n",
    "\n",
    "custom_meta_key_fields = 'metafields'\n",
    "custom_meta_key = 'metaset'\n",
    "\n",
    "schemaname = 'osc_corp_data'\n",
    "cur.execute('create schema if not exists osc_datacommons_dev.' + schemaname)\n",
    "cur.fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4447cdbd-4b5b-42b5-9386-bac8f52c8981",
   "metadata": {},
   "source": [
    "For osc_datacommons_dev, a trino pipeline is a parquet data stored in the S3_DEV_BUCKET\n",
    "It is a 5-step process to get there from a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75766c69-167c-4100-8811-ce8c49870759",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trino_pipeline (s3, schemaname, tablename, timestamp, df, meta_fields, meta_content):\n",
    "    global ingest_uuid\n",
    "    global custom_meta_key_fields, custom_meta_key\n",
    "    \n",
    "    # First convert dataframe to pyarrow for type conversion and basic metadata\n",
    "    table = pa.Table.from_pandas(enforce_sql_column_names(df))\n",
    "    # Second, since pyarrow tables are immutable, create a new table with additional combined metadata\n",
    "    if meta_fields or meta_content:\n",
    "        meta_json_fields = json.dumps(meta_fields)\n",
    "        meta_json = json.dumps(meta_content)\n",
    "        existing_meta = table.schema.metadata\n",
    "        combined_meta = {\n",
    "            custom_meta_key_fields.encode(): meta_json_fields.encode(),\n",
    "            custom_meta_key.encode(): meta_json.encode(),\n",
    "            **existing_meta\n",
    "        }\n",
    "        table = table.replace_schema_metadata(combined_meta)\n",
    "    # Third, convert table to parquet format (which cannot be written directly to s3)\n",
    "    pq.write_table(table, '/tmp/{sname}.{tname}.{uuid}.{timestamp}.parquet'.format(sname=schemaname, tname=tablename, uuid=ingest_uuid, timestamp=timestamp))\n",
    "    # df.to_parquet('/tmp/{sname}.{tname}.{uuid}.parquet'.format(sname=schemaname, tname=tablename, uuid=ingest_uuid, index=False))\n",
    "    # Fourth, put the parquet-ified data into our S3 bucket for trino.  We cannot compute parquet format directly to S3 but we can copy it once computed\n",
    "    s3.upload_file(\n",
    "        Bucket=os.environ['S3_DEV_BUCKET'],\n",
    "        Key='trino/{sname}/{tname}/{uuid}/{timestamp}/{tname}.parquet'.format(sname=schemaname, tname=tablename, uuid=ingest_uuid, timestamp=timestamp),\n",
    "        Filename='/tmp/{sname}.{tname}.{uuid}.{timestamp}.parquet'.format(sname=schemaname, tname=tablename, uuid=ingest_uuid, timestamp=timestamp)\n",
    "    )\n",
    "    # Finally, create the trino table backed by our parquet files enhanced by our metadata\n",
    "    cur.execute('.'.join(['drop table if exists osc_datacommons_dev', schemaname, tablename]))\n",
    "    print('dropping table: ' + tablename)\n",
    "    cur.fetchall()\n",
    "    \n",
    "    schema = create_table_schema_pairs(df)\n",
    "\n",
    "    tabledef = \"\"\"create table if not exists osc_datacommons_dev.{sname}.{tname}(\n",
    "{schema}\n",
    ") with (\n",
    "    format = 'parquet',\n",
    "    external_location = 's3a://{bucket}/trino/{sname}/{tname}/{uuid}/{timestamp}'\n",
    ")\"\"\".format(schema=schema,bucket=os.environ['S3_DEV_BUCKET'],sname=schemaname,tname=tablename,uuid=ingest_uuid,timestamp=timestamp)\n",
    "    print(tabledef)\n",
    "\n",
    "    # tables created externally may not show up immediately in cloud-beaver\n",
    "    cur.execute(tabledef)\n",
    "    cur.fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca5e83d-618d-43c0-aeb1-d0bc0706476d",
   "metadata": {},
   "source": [
    "### Write out Report with metadata\n",
    "\n",
    "Create the actual metadata for the source.  In this case, it is osc_corp_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176a9c61-9180-446e-80b3-0cc95d34c4d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "custom_meta_content = {}\n",
    "metadata_text = \"\"\"Title: AEP GHG and Energy Report, 2020\n",
    "Description: \n",
    "Version: 2020\n",
    "Release Date: \n",
    "URI: https://reports.shell.com/sustainability-report/2020/our-performance-data/greenhouse-gas-and-energy-data.html\n",
    "Copyright: \n",
    "License: \n",
    "Contact: \n",
    "Citation: \"\"\"\n",
    "\n",
    "for line in metadata_text.split('\\n'):\n",
    "    k, v = line.split(':', 1)\n",
    "    k = sql_compliant_name(k)\n",
    "    custom_meta_content[k] = v\n",
    "\n",
    "custom_meta_content['abstract'] = \"\"\"Abstract text\"\"\"\n",
    "custom_meta_content['name'] = 'osc_corp_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b7a3fe-4149-45e4-88b9-9bd3a7bd009d",
   "metadata": {},
   "source": [
    "Create the metadata for all the fields in all the tables"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7bd5046d-5237-42b9-9d40-508bfd177bcf",
   "metadata": {},
   "source": [
    "field_text = \"\"\"`country` (text): 3 character country code corresponding to the ISO 3166-1 alpha-3 specification [https://www.iso.org/iso-3166-country-codes.html]\n",
    "`country_long` (text): longer form of the country designation\n",
    "`name` (text): name or title of the power plant, generally in Romanized form\n",
    "`gppd_idnr` (text): 10 or 12 character identifier for the power plant\n",
    "`capacity_mw` (number): electrical generating capacity in megawatts\n",
    "`latitude` (number): geolocation in decimal degrees; WGS84 (EPSG:4326)\n",
    "`longitude` (number): geolocation in decimal degrees; WGS84 (EPSG:4326)\n",
    "`primary_fuel` (text): energy source used in primary electricity generation or export\n",
    "`other_fuel1` (text): energy source used in electricity generation or export\n",
    "`other_fuel2` (text): energy source used in electricity generation or export\n",
    "`other_fuel3` (text): energy source used in electricity generation or export\n",
    "`commissioning_year` (number): year of plant operation, weighted by unit-capacity when data is available\n",
    "`owner` (text): majority shareholder of the power plant, generally in Romanized form\n",
    "`source` (text): entity reporting the data; could be an organization, report, or document, generally in Romanized form\n",
    "`url` (text): web document corresponding to the `source` field\n",
    "`geolocation_source` (text): attribution for geolocation information\n",
    "`wepp_id` (text): a reference to a unique plant identifier in the widely-used PLATTS-WEPP database.\n",
    "`year_of_capacity_data` (number): year the capacity information was reported\n",
    "`generation_data_source` (text): attribution for the reported generation information\"\"\"\n",
    "\n",
    "field_descs = [line.split(': ')[1] for line in field_text.split('\\n')]\n",
    "field_keys = [line.split(': ')[0].split(' ')[0][1:-1] for line in field_text.split('\\n')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d566b9fd-7cfa-42da-a5f5-b0be69d0e85e",
   "metadata": {},
   "source": [
    "Create custom meta data and key"
   ]
  },
  {
   "cell_type": "raw",
   "id": "93a27bcc-cb92-481a-bf99-ba9ac9090a9c",
   "metadata": {},
   "source": [
    "custom_meta_fields = {}\n",
    "for k, v in zip(field_keys, field_descs):\n",
    "    custom_meta_fields[k] = { 'description': v }\n",
    "\n",
    "custom_meta_fields['capacity_mw']['dimension'] = 'MW'\n",
    "custom_meta_fields['latitude']['dimension'] = 'degrees'\n",
    "custom_meta_fields['longitude']['dimension'] = 'degrees'\n",
    "custom_meta_fields['commissioning_year']['dimension'] = 'year'\n",
    "custom_meta_fields['year_of_capacity_data']['dimension'] = 'year'\n",
    "custom_meta_fields['year'] = { 'description': 'year of report', 'dimension': 'year'}\n",
    "custom_meta_fields['gppd_idnr'] = { 'description': 'unique index into plants table', 'dimension': None}\n",
    "custom_meta_fields['generation_gwh'] = { 'description': 'electricity generation in gigawatt-hours reported for the year', 'dimension': 'GWh'}\n",
    "custom_meta_fields['estimated_generation_gwh'] = { 'description': 'estimated electricity generation in gigawatt-hours reported for the year', 'dimension': 'GWh'}\n",
    "custom_meta_fields['estimated_generation_note'] = { 'description': 'label of the model/method used to estimate generation for the year', 'dimension': None }\n",
    "custom_meta_key_fields = 'metafields'\n",
    "\n",
    "custom_meta_content = {\n",
    "    'title': 'Global Power Plant Database',\n",
    "    'description': 'A comprehensive, global, open source database of power plants',\n",
    "    'version': '1.3.0',\n",
    "    'release_date': '20210602'\n",
    "}\n",
    "custom_meta_key = 'metaset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5e1321-bfa5-49a8-975e-d88042985316",
   "metadata": {},
   "outputs": [],
   "source": [
    "shell_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d722f0bd-eca3-4bbc-9406-26d6b09d7d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "tablename = 'aep_2020'\n",
    "custom_meta_fields = {}\n",
    "create_trino_pipeline (s3, schemaname, tablename, timestamp, shell_df, custom_meta_fields, custom_meta_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4b29ea-3636-4af6-8108-49f4d3c80cf4",
   "metadata": {},
   "source": [
    "Restore data and metadata"
   ]
  },
  {
   "cell_type": "raw",
   "id": "26c30d50-88f0-4f41-b1f8-1dffc87c21b1",
   "metadata": {},
   "source": [
    "# Read the Parquet file into an Arrow table\n",
    "obj = s3.get_object(\n",
    "    Bucket=os.environ['S3_DEV_BUCKET'], \n",
    "    Key='trino/{sname}/{tname}/{uuid}/{timestamp}/{tname}.parquet'.format(sname=schemaname, tname=tablename, uuid=ingest_uuid, timestamp=timestamp)\n",
    ")\n",
    "restored_table = pq.read_table(io.BytesIO(obj['Body'].read()))\n",
    "# Call the tables to_pandas conversion method to restore the dataframe\n",
    "# This operation uses the Pandas metadata to reconstruct the dataFrame under the hood\n",
    "restored_df = restored_table.to_pandas()\n",
    "# The custom metadata is accessible via the Arrow tables metadata object\n",
    "# Use the custom metadata key used earlier (taking care to once again encode the key as bytes)\n",
    "restored_meta_json = restored_table.schema.metadata[custom_meta_key.encode()]\n",
    "# Deserialize the json string to get back metadata\n",
    "restored_meta = json.loads(restored_meta_json)\n",
    "# Use the custom metadata fields key used earlier (taking care to once again encode the key as bytes)\n",
    "restored_meta_json_fields = restored_table.schema.metadata[custom_meta_key_fields.encode()]\n",
    "# Deserialize the json string to get back metadata\n",
    "restored_meta_fields = json.loads(restored_meta_json_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cd177b-4035-43ee-a8bf-6b8500be0612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Everything below here is speculative / in process of design"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bac6e28-686b-4540-bc9f-83b44cba3d29",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load metadata following an ingestion process into trino metadata store\n",
    "\n",
    "### The schema is *metastore*, and the table names are *meta_schema*, *meta_table*, *meta_field*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14c7bbc-c00f-41e8-85b0-6b553e73a24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create metastore structure\n",
    "metastore = {'catalog':'osc_datacommons_dev',\n",
    "             'schema':'aep_2020',\n",
    "             'table':tablename,\n",
    "             'metadata':custom_meta_content,\n",
    "             'uuid':ingest_uuid}\n",
    "# Create DataFrame\n",
    "df_meta = pd.DataFrame(metastore)\n",
    "# Print the output\n",
    "df_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44b86eb-aebb-445d-b125-197b7f73a83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(iam_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e01667-a1a2-4752-b756-dad449a8853d",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(registry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb585901-7a49-4bf2-aa6e-732ba4a8e98f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
